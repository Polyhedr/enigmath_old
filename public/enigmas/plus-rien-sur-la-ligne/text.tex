%algèbre
\section*{Plus rien sur la ligne}
\begin{center}
    \includegraphics[width=0.4\textwidth]{\currfiledir/image.jpg}
\end{center}
\subsection*{Énoncé}
On considère une ligne infinie de cases indexées par les entiers relatifs
\(\mathbb{Z}\triangleq\{
\dots, -2, -1, 0, 1, 2, \dots \}
\), initialement avec un seul jeton sur la case $0$, toutes les autres étant vides.
Au cours du jeu, chaque case peut contenir une pile quelconque de jetons, et l'objectif est de retirer tous les jetons de la ligne en appliquant une suite d'actions autorisées.  

Une action $A$ est définie comme une transformation appliquée à une portion de la ligne, que l'on note
\[
A\triangleq(a_0, \dots, a_k) 
\longleftrightarrow  (a'_0, \dots, a'_k),
\]
où $k \in \mathbb{N},$ $a_0, \dots, a_k,a'_0, \dots, a'_k \in \mathbb{N}$ et $(a_0, \dots, a_k)\neq (a'_0, \dots, a'_k)$. Pour \(i\in\mathbb Z\), appliquer \(A\) sur les \(k+1\) cases consécutives d'indices \(i,\dots,i+k\) signifie qu'on retire \(a_j\) jetons de la case \(i+j\) et qu'on ajoute \(a'_j\) jetons à la même case, pour tout \(0\le j\le k\). L'opération n'est permise que si chaque case \(i+j\) contient au moins \(a_j\) jetons, suffisants pour effectuer les retraits requis (aucun nombre négatif de jetons n'est autorisé, même temporairement). L'action $A$ peut également être effectuée dans le sens inverse en échangeant les $a_j$ et les $a'_j$.


\medskip
\textbf{Questions} :
\begin{enumerate}
\item  \indicators{0.4}{0} Résolvez le jeu en utilisant les deux actions autorisées :
$$
A\triangleq(1,3,1,0,0) \longleftrightarrow (0,2,1,1,2) \quad \text{et} \quad B\triangleq(0,0) \longleftrightarrow (1,1).
$$

\item\indicators{0.5}{0} Montrez que, pour que le jeu soit résoluble, il est nécessaire qu'une des actions autorisées ait un côté constitué uniquement de zéros.

\item\indicators{0.7}{0} Avec les deux actions
\[
A\triangleq(1,0,0,0) \longleftrightarrow (0,1,1,1) \quad \text{et} \quad B\triangleq(0,0) \longleftrightarrow (1,1),
\]
le jeu est-il résoluble ? 

\item\indicators{1.0}{0} Avec les deux actions
\[
A\triangleq(1,0,0,0,0,0) \longleftrightarrow (0,1,1,1,1,1) \quad \text{et} \quad B\triangleq(0,0,0) \longleftrightarrow (1,1,1),
\]
le jeu est-il résoluble ? 

\item\indicators{1.5}{0} 
À chaque action
\[
A\triangleq(a_0, \dots, a_k) \longleftrightarrow (a'_0, \dots, a'_k)
\]
on associe le polynôme
\[
A(X) \triangleq \sum_{j=0}^k (a'_j - a_j) X^{j} \in \mathbb{Z}[X].
\]
Montrez que, pour que le jeu soit résoluble, il est nécessaire et suffisant qu'au moins une des actions autorisées ait un côté constitué uniquement de zéros, et que l'idéal de \(\mathbb{Z}[X]\) engendré par les polynômes associés aux actions autorisées
  contienne une puissance de \(X\). 
\item\indicators{1.5}{0} On suppose que les coefficients constants des polynômes associés aux actions autorisées 
sont premiers entre eux dans leur ensemble. 
Trouvez alors une condition nécessaire et suffisante plus forte que la précédente.

\item\indicators{1.9}{0} On suppose maintenant qu'il y a exactement deux actions, 
auxquelles on associe les polynômes $P$ et $Q$, 
avec l'un des deux à coefficients positifs et correspondant à une action dont un côté est le vecteur nul.  
On suppose que \(P(0)\) et \(Q(0)\) sont premiers entre eux, 
et que le coefficient dominant de l’un des polynômes \(P\) ou \(Q\) appartient à \(\{\pm 1\}\).  Montrez alors qu’une condition nécessaire et suffisante de résolubilité du jeu 
est que le résultant 
\(
  \operatorname{Res}(P,Q)
\)
soit égal à \(\pm 1\).

\item\indicators{2.4}{0} Soient \(P\) et \(Q\) deux polynômes de $\mathbb Z[X]$ dont les coefficients sont croissants,  
avec \(P\) à coefficients positifs et \(Q\) à coefficients négatifs.
On suppose que le coefficient dominant de \(P\) est strictement supérieur à \(1\), 
et que le coefficient dominant de \(Q\) est égal à \(-1\).  
Montrez que le jeu associé aux polynômes $P(X)$ et $1 + XQ(X)$
n'est pas résoluble.
  
\item\indicators{1.9}{0} Soient 
\(
a=(1,0,\dots,0)\in \{0,1\}^{q+1} 
\) et \(
b=(0,\dots,0)\in \{0,1\}^{p}.
\)
Avec les deux actions
\[
A \triangleq a \longleftrightarrow 1-a 
\qquad \text{et} \qquad 
B \triangleq b \longleftrightarrow 1-b,
\]
montrez que le jeu est résoluble si et seulement si $p$ divise $q$.

\item\indicators{1.5}{0} On considère une grille infinie de cases indexées par les paires \((i,j) \in \mathbb{Z} \times \mathbb{Z}\).  
Initialement, un unique jeton est placé en \((0,0)\), toutes les autres cases étant vides.  
On définie une action $A$ par
\[
A \triangleq 
\begin{bmatrix}
a_{00} & \cdots & a_{0k} \\
\vdots & \ddots & \vdots \\
a_{k0} & \cdots & a_{kk}
\end{bmatrix}
\longleftrightarrow
\begin{bmatrix}
a'_{00} & \cdots & a'_{0k} \\
\vdots & \ddots & \vdots \\
a'_{k0} & \cdots & a'_{kk}
\end{bmatrix}.
\]
Trouvez une condition nécessaire et suffisante pour que le jeu soit résoluble dans cette grille bidimensionnelle, en fonction de l’ensemble d’actions autorisées.

\item\indicators{1.4}{0} Avec les deux actions (de taille non nécessairement égale)
\[
A \triangleq 
\begin{bmatrix}
0 & 0 &\cdots & 0 \\
1 & 0 &\cdots & 0
\end{bmatrix}
\longleftrightarrow
\begin{bmatrix}
1 & 1 &\cdots & 1 \\
0 & 0 &\cdots & 0
\end{bmatrix}
\qquad \text{et} \qquad 
B \triangleq  \begin{bmatrix}
0 &\cdots & 0
\end{bmatrix}
\longleftrightarrow
\begin{bmatrix}
1 & \cdots & 1 
\end{bmatrix},
\]
le jeu est-il résoluble ?
\end{enumerate}

\subsection*{Solution}
\begin{enumerate}
\item 
En notant $B^{-1}$ le fait d'appliquer l'action $B$ en sens inverse, on a :
\begin{align*}(\dots,0,0,1,0,0,0,0,\dots)&\overset{B}{\to}(\dots,0,1,2,0,0,0,0,\dots)\overset{B}{\to}(\dots,0,1,3,1,0,0,0,\dots)
\\\overset{A}{\to}
(\dots,0,0,2,1,1,2,0,\dots)
&\overset{B}{\to}
(\dots,0,0,2,2,2,2,0,\dots)
\overset{B^{-1}}{\to}
(\dots,0,0,1,1,2,2,0,\dots)
\\\overset{B^{-1}}{\to}
(\dots,0,0,1,1,1,1,0,\dots)
&\overset{B^{-1}}{\to}
(\dots,0,0,0,0,1,1,0,\dots)
\overset{B^{-1}}{\to}
(\dots,0,0,0,0,0,0,0,\dots).
\end{align*}

\item \label{Montrez que, pour que le jeu soit résoluble} Supposons qu’une suite finie de coups mène à la configuration nulle, et considérons
le dernier coup, appliqué sur un bloc $I$. Juste avant, tous les jetons étaient
contenus dans $I$, dont la configuration correspondait à l’un des deux motifs
de l’action. Pour que le coup produise la configuration nulle, l’autre motif
doit être le vecteur nul.
\item La parité du nombre total de jetons est invariante par les actions et vaut toujours 1 (jeton initial), donc la configuration nulle est inaccessible.
\item  On note $\mathbb{F}_2 = \{0,1\}$ le corps fini à 2 éléments.
Soit $\omega$ une racine primitive cubique de l'unité, i.e. $1+\omega+\omega^2=0$.
Pour une configuration $a=(a_n)_{n\in\mathbb Z}$ de jetons (à support fini), posons la somme pondérée
$S=\sum_{n\in\mathbb Z} a_n\,\omega^n\in\mathbb{F}_2[\omega].$
Nous examinons l'effet des actions autorisées sur $S$. L'action~$B$ ajoute
$\omega^i+\omega^{i+1}+\omega^{i+2}=0$ à $S$ et 
l'action $A$ ajoute $-\omega^i+\omega^{i+1}+\omega^{i+2}+\omega^{i+3}+\omega^{i+4}+\omega^{i+5}=-2\omega^i=0$ à $S.$
Par conséquent, $S$ reste invariante au cours du jeu.
Or, la valeur de $S$ sur la configuration initiale (un seul jeton en position $0$) vaut $1$,
alors que celle de la configuration vide vaut $0$.  D'o\`u la non-résolubilité du jeu. 

On peut également prouver l'impossibilité en attribuant aux cases les étiquettes périodiques $(\omega_i) = (\dots, 1, 1, 0, 1, 1, 0, \dots)$, avec $\omega_0 = 1$, et en montrant que la parité de $S = \sum_{n \in \mathbb{Z}} a_n \omega_n$ est un invariant sous les transformations autorisées.
\item \label{À chaque action}
On note $A_1, \dots, A_m$ les actions autorisées.
On attribue aux cases les étiquettes $X^{n}$ pour $n \in \mathbb{Z}$. On représente la configuration $a=(a_n)_{n\in\mathbb Z}$ par le polynôme de Laurent $S = \sum_{n \in \mathbb{Z}} a_n X^n$.  Appliquer l'action $A_{\ell}$ en position $i\in \mathbb Z$ (i.e., aux cases $i,\dots,i+\text{deg(A)}$) ajoute $X^{i}A_{\ell}(X)$ à $S$ (en sens inverse, on retranche $X^{i}A_{\ell}(X)$ à $S$). 

Commençons par montrer que les deux conditions sont suffisantes. 
On dispose d'une identité
\[
  0 \;=\; X^q + \sum_{\ell=1}^m A_\ell(X)\Biggl(\sum_{j=0}^{d_{\ell}} c_{\ell,j}\,X^{j}\Biggr),
\]
où l'on suppose \(0\neq A_1\in\mathbb{N}[X]\). En divisant par \(X^q\) on obtient
\[
  -1 \;=\; \sum_{\ell=1}^m A_\ell(X)\Biggl(\sum_{j=0}^{d_{\ell}} c_{\ell,j}\,X^{\,j-q}\Biggr).
\]
L'interprétation combinatoire est la suivante : pour chaque paire \((\ell,j)\), 
il faut appliquer l'action \(A_\ell\) exactement \(|c_{\ell,j}|\) fois en position $j-q$
(dans le sens direct si \(c_{\ell,j}>0\), dans le sens inverse si \(c_{\ell,j}<0\)). 
Par l'identité ci-dessus, ces actions annulent la configuration initiale. 
Cependant, certaines de ces applications peuvent être a priori impossibles faute de jetons. 
Comme \(A_1~\neq~0\) possède uniquement des coefficients positifs, 
on peut choisir un polynôme de Laurent \(P=\sum_{j\in \mathbb Z} p_jX^j\) à coefficients dans \(\mathbb{N}\) et appliquer préalablement \(P(X)A_1(X)\) 
(c'est-à-dire appliquer $p_j$ fois \(A_1\) en position $j$, pour $j\in \mathbb Z$)  
de sorte qu'il y ait suffisamment de jetons et que toutes les actions prévues deviennent réalisables. 
Après les avoir appliquées, on obtient \(S = P(X)A_1(X)\).  
En appliquant ensuite \(A_1\) en sens inverse autant de fois qu'au départ, 
on retire les jetons ajoutés et on obtient la configuration nulle. 


La première condition a déjà été démontrée nécessaire à la question~\ref{Montrez que, pour que le jeu soit résoluble}. Il reste donc à établir la nécessité de la seconde condition. 
Supposons qu’une suite finie de $T$ coups mène à la configuration nulle : à l'instant $1\le t\le T$, on applique l'action $A_{\ell_t}$ en position $i_t$ (dans le sens direct si $\varepsilon_t=1$ et dans le sens inverse si $\varepsilon_t=-1$). On a alors la relation :
\[
0 = 1 + \sum_{t=1}^T \varepsilon_t X^{i_t}A_{\ell_t}(X) = 1 + \sum_{\ell=1}^m A_{\ell}(X) \left(\sum_{t=1}^T \varepsilon_t X^{i_t} \mathbb{I}\{\ell_t=\ell\}\right),
\]
où $\mathbb{I}$ est la fonction indicatrice. En multipliant par $X^q$, pour $q$ suffisamment grand afin d’éliminer les puissances négatives, 
on obtient une relation de Bézout dans $\mathbb{Z}[X]$ :
\[
X^q \;=\; \sum_{\ell=1}^m A_{\ell}(X)\,U_{\ell}(X),
\]
avec $U_{1}(X),\dots,U_m(X)\in\mathbb{Z}[X]$. 
\item \label{On suppose que les coefficients constants des polynômes associés aux actions autorisées 
sont premiers entre eux dans leur ensemble.} On repart de l'identité
\[
  X^q \;=\; \sum_{\ell=1}^m A_{\ell}(X)\,U_{\ell}(X).
\]
Par l'hypothèse sur les coefficients constants, il existe 
$\alpha_1,\dots,\alpha_m \in \mathbb{Z}$ et $P\in\mathbb{Z}[X]$ tels que
\[
  \sum_{\ell=1}^m \alpha_{\ell} A_{\ell}(X) + 1 \;=\; X P(X).
\]
Ainsi, on obtient
\[
  \sum_{\ell=1}^m A_{\ell}(X)\,U_{\ell}(X)P(X)^q 
  \;=\; (X P(X))^q
  \;=\; \Biggl(\sum_{\ell=1}^m \alpha_{\ell} A_{\ell}(X) + 1\Biggr)^q
  \;=\; 1 \,+\, Q(X)\sum_{\ell=1}^m \alpha_{\ell} A_{\ell}(X),
\]
avec $Q\in \mathbb{Z}[X]$.  
On en déduit que l'idéal engendré par les polynômes associés aux actions autorisées vaut \(\mathbb{Z}[X]\) tout entier.
\item \label{On suppose maintenant qu'il y a exactement deux actions}
Si \(\operatorname{Res}(P,Q)=\pm1\), alors, comme le résultant appartient à l'idéal engendré par \(P\) et \(Q\), 
cet idéal est \(\mathbb{Z}[X]\) tout entier. 
Compte tenu des hypothèses de l'énoncé, cela fournit une condition suffisante de résolubilité, d'après la question~\ref{À chaque action}.

Réciproquement, si le jeu est résoluble, on obtient par la question~\ref{On suppose que les coefficients constants des polynômes associés aux actions autorisées 
sont premiers entre eux dans leur ensemble.} (en utilisant que $P(0)$ et $Q(0)$ sont premiers entre eux) une identité de Bézout 
\(
UP+VQ=1.
\)
En supposant que \(Q\) a son coefficient dominant dans \(\{\pm1\}\), et en utilisant les propriétés du résultant, on calcule
\[
\pm1 \;=\; \operatorname{Res}(1,Q)
     \;=\; \operatorname{Res}\big(UP+VQ,Q\big)
     \;=\; \pm\,\operatorname{Res}(UP,Q)
     \;=\; \pm\,\operatorname{Res}(U,Q)\,\operatorname{Res}(P,Q).
\]
Ainsi, \(\operatorname{Res}(P,Q)\) divise \(\pm1\) dans \(\mathbb{Z}\), donc \(\operatorname{Res}(P,Q)=\pm1\).
\item 
Pour 
\(
A(X)\triangleq a_n X^n + a_{n-1} X^{n-1} + \cdots + a_0, \) et \(
B(X)\triangleq b_m X^m + b_{m-1} X^{m-1} + \cdots + b_0,
\)
on définit la matrice de Sylvester \(M\) associée à \((A,B)\) par
\[
M \triangleq
\begin{pmatrix} 
a_n     & 0      & \cdots & 0      & b_m     & 0      & \cdots & 0      \\
a_{n-1} & a_n    & \ddots & \vdots & \vdots & b_m    & \ddots & \vdots \\
\vdots  & a_{n-1}& \ddots & 0      & \vdots & \vdots & \ddots & 0      \\
\vdots  & \vdots & \ddots & a_n    & b_1    &        &        & b_m    \\
a_0     &        &        & a_{n-1}& b_0    & \ddots & \vdots & \vdots \\
0       & \ddots & \vdots & \vdots & 0      & \ddots & b_1    & \vdots \\
\vdots  & \ddots & a_0    & \vdots & \vdots & \ddots & b_0    & b_1    \\
0       & \cdots & 0      & a_0    & 0      & \cdots & 0      & b_0
\end{pmatrix}.
\]
Par définition, le résultant est
\(
\operatorname{Res}(A,B) = \det(M).
\)
On souhaite montrer que, dans notre cas, le résultant est supérieur ou égal au coefficient dominant de \(P\). Par exemple,
\[
\operatorname{Res}(2X+1,\,1-X)
=
\begin{vmatrix} 
2 & -1 \\
1 & 1 
\end{vmatrix}
= 3 \ge 2.
\]
Cette condition implique que le résultant n'est pas \(\pm 1\), ce qui suffit à garantir la non-résolubilité par la question~\ref{On suppose maintenant qu'il y a exactement deux actions} (le coefficient constant de \(1 + X Q(X)\) vaut \(1\), donc est premier avec celui de~\(P\), et le coefficient dominant vaut \(-1\)).
Pour ce faire, on considère la décomposition LU de la matrice de Sylvester. 
Plus précisément, le lemme suivant définit une famille de matrices vérifiant le résultat souhaité, dont la matrice de Sylvester fait partie par hypothèse sur $P$ et $Q$.

\begin{lemma}
Soit $\mathcal{F}_n$ la famille de matrices $M$ carrées de taille $n \times n$, à coefficients dans $\mathbb{R}$, telles que, pour tout $j \in \{1,\dots,n\}$~:
\begin{align*}
&(M_{ij})_{i} \text{ est négative décroissante pour } i\in \{1,\dots,j-1\},\\
&M_{jj} \geq 1,\\ 
&(M_{ij})_{i} \text{ est positive décroissante pour } \{j,\dots,n\}.
\end{align*}
Alors, pour tout $M \in \mathcal{F}_n$, $\det(M)\geq \prod_{j=1}^nM_{jj} \geq M_{11}$.
\end{lemma}

\begin{proof}
On effectue l’élimination gaussienne de $M \in \mathcal{F}_n$ pour annuler tous les coefficients situés au-dessus de la diagonale, transformant $M$ en matrice triangulaire inférieure.  
Si l’élimination n’est pas terminée, il existe un $(i,j)$ avec $i<j$ et $M_{ij}<0$. On choisit le plus petit $j$, puis le plus petit $i$ correspondant. On ajoute alors à la $j$-ème colonne $\frac{-M_{ij}}{M_{ii}}$ fois la $i$-ème colonne.  
Cette opération ne modifie pas le déterminant et la matrice $M'$ ainsi obtenue reste dans $\mathcal{F}_n$. En effet, les autres colonnes restent inchangées et la $j$-ème colonne vaut
$\bigl(\frac{-M_{ij}}{M_{ii}} M_{k i} + M_{k j}\bigr)_{1\le k\le n}.$
Ainsi, pour \(k\in\{i,\dots,j-1\}\), nous avons la somme de deux suites décroissantes dont le premier terme est nul, ce qui donne une suite négative décroissante. Comme les termes sont nuls pour \(k\in \{1,\dots,i-1\}\), la suite reste négative décroissante pour \(k\in \{1,\dots,j-1\}\). Pour \(k = j\), comme \(M_{ji} \ge 0\) (car \(j \ge i\)), on a \(M'_{jj} \ge M_{jj} \ge 1\). Pour \(k\in \{j,\dots,n\}\), nous avons la somme de deux suites positives décroissantes, ce qui donne une suite positive décroissante.

Par récurrence sur les étapes de l’élimination, le déterminant est égal au produit des coefficients diagonaux après la dernière étape,  
chacun d’eux étant supérieur ou égal au coefficient diagonal initial correspondant. 
Ces coefficients initiaux étant tous supérieurs à $1$, le résultat s'ensuit.
\end{proof}
\item \label{Soient} Si $p$ divise $q$, on peut clairement résoudre le jeu en appliquant $A$, puis $q/p$ fois $B^{-1}$.  
Sinon, écrivons la division euclidienne de $q$ par $p$ sous la forme
$
q = pm + r, $ avec $ 0 < r < p.
$
Considérons alors le jeu avec les deux actions
\[
Q \triangleq (0,\underbrace{1,\dots,1}_{r \text{ fois}}) 
\longleftrightarrow (1,0,\dots,0),
\qquad
P \triangleq (0,\dots,0) 
\longleftrightarrow (\underbrace{1,\dots,1}_{p-r-1 \text{ fois}}, 2).
\]
On remarque que l’on peut reconstruire les actions $A$ et $B$ à partir de $P$ et $Q$ : l’action $B$ s’obtient en effectuant $P$ puis $Q^{-1}$, l’action $A$ s’obtient en effectuant $Q^{-1}$ puis $m$ fois $B$. 
Or, d’après la question précédente, le jeu avec $P$ et $Q$ n’est pas résoluble. Il en résulte que le jeu avec $A$ et $B$ n’est pas résoluble non plus.
\item 
Par généralisation directe de la question~\ref{À chaque action}, on obtient la condition nécessaire et suffisante 
suivante : il faut qu’au moins une des actions autorisées possède un côté constitué uniquement de 
zéros, et que l’idéal de \(\mathbb{Z}[X,Y]\) engendré par les polynômes (de deux variables) associés aux actions autorisées 
contienne un monômes \(X^iY^j\) pour $i,j\in \mathbb N$.
\item 
On note $q$ (resp. $p$) la dimension horizontale de $A$ (resp. $B$). Le jeu est résoluble si et seulement si $p$ divise $q$. 
En effet, si $p \mid q$, il est clair que l'on peut résoudre le jeu en appliquant d'abord $A$, puis $q/p$ fois $B^{-1}$. 
Supposons maintenant, par l'absurde, que le jeu soit résoluble alors que $p \nmid q$. D'après la question précédente, il existerait des polynômes $U(X,Y)$ et $V(X,Y)$ tels que
\[
X^i Y^j = U(X,Y) A(X,Y) + V(X,Y) B(X,Y).
\]
En évaluant en $X=Y$, on obtient que le jeu considéré à la question~\ref{Soient}, avec les mêmes valeurs de $p$ et $q$, serait résoluble, ce qui est une contradiction. 
 


\end{enumerate}
\subsection*{Notes et références}
L’énigme s’inscrit dans une vaste famille de jeux combinatoires étudiés sur des structures
telles que les lignes, les grilles ou, plus généralement, les graphes. Ces jeux ont en commun 
des règles locales simples qui, lorsqu’elles sont appliquées de manière répétée, 
donnent lieu à des comportements globaux riches et parfois difficiles à anticiper. 
Parmi les exemples classiques, on peut mentionner : 

\begin{itemize}
    \item le \emph{solitaire} classique \cite{berlekamp2001winning}, avec ses nombreuses variantes 
    comme le \emph{multichip peg solitaire}, ou encore sa généralisation 
    sur les graphes infinis \cite{vito2021peg} ;
    
    \item le jeu électronique \emph{Lights Out} \cite{anderson1998turning}, apparu dans les années 1990,
    où l’appui sur une case d’une grille modifie simultanément l’état de cette case et de ses voisines,
    ce qui conduit à une analyse naturelle en termes d’algèbre linéaire sur les corps finis ;
    
    \item le \emph{chip-firing game} (ou \emph{jeu de répartition de jetons}) \cite{biggs1999chip},
    consistant à redistribuer des jetons sur les sommets d’un graphe selon des règles de
    \emph{tir} locales, et qui est intimement lié au problème du comptage
    des arbres couvrants ;
    
    \item les \emph{automates cellulaires}, dont l’exemple emblématique est le \emph{jeu de la Vie}
    (\emph{Game of Life}) de Conway \cite{conway1970game}, illustrant comment des règles locales 
    très simples peuvent engendrer des comportements globaux complexes et imprévisibles ;
    
    \item le problème des \emph{Conway’s Soldiers} \cite{berlekamp2001winning}, variante du solitaire 
    sur un damier infini : les pions (soldats) occupent toutes les cases situées en dessous d’une ligne horizontale, 
    et chaque coup consiste à faire sauter un soldat par-dessus un autre, horizontalement ou verticalement, 
    en retirant celui qui est sauté. L’objectif est de parvenir à placer un soldat aussi haut que possible au-dessus de la ligne. 
\end{itemize}


\bibliography{\currfiledir/sources.bib}
\newpage